# 🤯 1,000만 건이라며? 왜 4분밖에 안 걸려? (스위스 시계 같은 Spark ㄷㄷ)

> "10배 많은 데이터를 줬는데, 시간은 1.3배만 늘어났습니다. 이거 실화인가요?" 🔥

![](https://velog.velcdn.com/images/tags/spark.png)

## 🎯 TL;DR

- ✅ **10,000,000건** 데이터 처리 성공 (드디어 천만 가즈아!)
- ⏱️ **실행 시간: 233초 (~3.9분)**
- 🚀 **놀라운 효율**: 100만 건 때보다 처리 속도가 7.6배 빨라짐
- 💾 **메모리: 157 MB** (여전히 평온함)
- 📊 **스케일링의 역설**: 데이터 10배 증가 → 시간 1.3배 증가

---

## 💭 Phase 4: 천만 건의 벽에 도전하다

지난 Phase 3에서 100만 건을 3분 만에 처리하면서 Spark의 "Sub-linear Scaling"을 확인했었죠. 하지만 1,000만 건은 단위가 다릅니다. 아무리 M2 Pro라도 로컬에서 천만 건은 좀 빡세지 않을까? 

**"이번엔 진짜 OOM(Out Of Memory) 한번 보겠지?"** 😈

라는 기분으로 데이터를 생성했습니다.

---

## 🎬 실행 과정 (The Challenge)

### 1. 데이터 생성

```bash
uv run python data_generator/generate_all.py --records 10000000
```

생성하는 데만 시간이 꽤 걸렸습니다. 약 678억 원 규모의 가상 거래 데이터... 내 통장에 있었으면... 💸

### 2. 벤치마크 실행

```bash
uv run python benchmark/run_pipeline_benchmark.py
```

자, 이제 맥북 팬이 돌기 시작합니다. (사실 조용했음)

---

## 📊 결과: 예측을 비웃는 압도적 성능

### 100만 vs 1,000만 비교

| 항목 | 100만 건 (P3) | 1,000만 건 (P4) | 변화율 |
| :--- | :--- | :--- | :--- |
| **처리 시간** | 179초 (~3.0분) | **233초 (~3.9분)** | **1.3배 증가** |
| **메모리 증가량** | 162 MB | **157 MB** | **오히려 감소(?)** |
| **처리율 (sec)** | 5,584 records/s | **42,775 records/s** | **7.6배 상승!** 🔥 |

**아니, 10배 많은 데이터를 줬는데 54초밖에 안 더 걸렸다고?** 😱

이 정도면 거의 "데이터가 많을수록 즐거워하는" 변태적인(?) 성능입니다.

---

## 🤔 왜 이렇게 빠른 거야? (Deep Dive)

### 1. "고정 오버헤드"의 희석 (The Amortization)

이제 명확해졌습니다. Spark는 엔진을 예열하는 데(SparkSession, Metadata, JVM) 약 100~150초를 고정적으로 씁니다. 
- 데이터가 적을 땐 엔진 예열 시간이 전체의 90%
- 천만 건쯤 되니까 예열 시간은 그대로고 실제 연산 비중이 늘어난 것뿐!

### 2. 메모리 사용의 마법 💾

메모리가 300MB는 넘을 줄 알았는데, 여전히 160MB 선에서 머물고 있습니다. 
- **Broadcast Join**: Dimension 테이블(날짜, 카테고리 등)은 여전히 작아서 메모리에 한 번만 올리면 끝.
- **Delta Lake**: 압축과 인덱싱이 워낙 잘 돼 있어서 I/O가 효율적.

### 3. 스케일링 성능 (Processing Throughput)

그래프를 그려보면 처리 속도가 기하급수적으로 올라가는 걸 볼 수 있습니다. 
- **1만 건**: 86 records/s
- **10만 건**: 896 records/s
- **100만 건**: 5,584 records/s
- **1,000만 건**: **42,775 records/s** ✅

---

## 💡 이번 Phase에서 배운 점

### 1. "빅데이터" 프레임워크는 이름값을 한다

작은 데이터를 다룰 땐 번거롭기만 하던 Spark가, 천만 건 단위로 가니까 무시무시한 효율을 보여줍니다. "이래서 Spark, Spark 하는구나" 싶네요.

### 2. 로컬 개발 환경의 한계는 아직 멀었다

맥북 M2 Pro 32GB 모델인데, 천만 건 정도는 간에 기별도 안 가는 것 같습니다. (다음엔 1억 건 가야 하나...?)

---

## 🎯 다음은? (Next Roadmap)

원래는 이다음에 "최적화"를 하려고 했습니다. 
근데 지금 성능이 너무 잘 나와서 **"최적화를 할 명분이 없어졌어요."** ㅋㅋㅋ

그래도 대용량 처리를 위한 기술적 장치들을 추가해 보려 합니다.

1. **Partitioning**: 날짜별로 데이터를 쪼개서 저장하기
2. **Spark Config 튜닝**: 메모리 구조를 뜯어서 더 효율적으로 만들기
3. **1억 건 도전?**: 진짜 맥북이 비명을 지르는지 확인하기

---

## 📝 마무리하며

천만 건을 4분 만에 끝내는 걸 보면서, 데이터 엔지니어링에서 프레임워크의 특성을 이해하는 게 얼마나 중요한지 깨달았습니다.

**"데이터가 늘어난다고 겁먹지 마세요. Spark는 데이터가 많을수록 더 신나게 일하니까요!"** 😎

---

**Tags**: `#Spark` `#DataEngineering` `#BigData` `#M2Pro` `#성능측정` `#Velog`

**다음 글 예고**: 1억 건 가즈아! 그리고 파티셔닝의 마법 🪄
