# 🚀 1억 건의 벽을 넘다: 로컬 PC에서 11분 만에 끝내는 Spark 최적화의 마법

> "데이터 10배 늘렸는데 시간은 3배? 이게 바로 '진짜' 데이터 엔지니어링의 묘미죠!" 🏆

![](https://velog.velcdn.com/images/tags/spark_optimized.png)

## 🎯 TL;DR

- ✅ **100,000,000건** 데이터 처리 성공 (드디어 1억 가즈아!)
- ⏱️ **실행 시간: 686초 (~11.4분)**
- 🛑 **위기**: 초기 시도 시 OOM(Out Of Memory) 및 Shuffle 에러로 실패
- 🛠️ **해결**: 계층적 파티셔닝 + Delta Lake Z-Order + Spark 메모리 튜닝
- 📊 **압도적 스케일링**: 1,000만 건 vs 1억 건 비교 시 데이터는 **10배**, 시간은 단 **2.9배** 증가!

---

## 🛑 Phase 5: "1억 건은 루비콘 강이었습니다"

지난 Phase 4에서 1,000만 건을 4분 만에 처리하며 기세등등했었죠. 하지만 1억 건은 차원이 달랐습니다. 

**"그냥 하던 대로 하면 되겠지?"** 🤨

라고 생각하며 돌린 첫 벤치마크는 저에게 **`Exit code: 1 (OOM/Shuffle Error)`**라는 처참한 결과를 안겨주었습니다. 4GB가 넘는 원천 데이터와 수십 GB의 셔플 데이터를 기본 설정으로 감당하기엔 로컬 환경의 한계가 명확했습니다.

---

## 🛠️ 해결책: "기본기"와 "고급 기술"의 조화

벽을 만났으니, 이제 진짜 "최적화"를 꺼내 들 때입니다.

### 1. 계층적 파티셔닝 (Hierarchical Partitioning) 🧱
데이터를 단순히 쌓는 게 아니라 `year`, `month` 컬럼을 생성해 물리적으로 나눴습니다. 
- **효과**: Spark이 필요한 시점의 데이터만 읽게 되어 I/O 부하가 획기적으로 줄어듭니다.

### 2. Delta Lake Z-Order 최적화 🪄
`transaction_id` 컬럼을 기준으로 데이터를 물리적으로 재배치했습니다.
- **효과**: 데이터 스키핑(Data Skipping)이 극대화되어, 조인(Join)과 필터링 속도가 비약적으로 상승합니다.

### 3. Spark 세션 집중 케어 🔧
- `driver.memory`: 8g / `executor.memory`: 16g로 대폭 증설.
- `shuffle.partitions`: 200(기본값)에서 400으로 늘려 셔플 데이터 부하 분산.
- `autoBroadcastJoinThreshold`: 작은 차원 테이블 조인 시 브로드캐스트 활성화.

---

## 📊 결과: 최적화가 만든 기적의 그래프

### 1,000만 vs 1억 (최적화 전 vs 후)

| 항목 | Phase 4 (1,000만) | Phase 5 (1억 - Optimized) | 변화율 |
| :--- | :--- | :--- | :--- |
| **레코드 수** | 10,000,000 | **100,000,000** | **10배 증가** |
| **처리 시간** | 233초 (~3.9분) | **686초 (~11.4분)** | **2.9배 증가** 🚀 |
| **초당 처리량** | 42,775 rec/s | **145,655 rec/s** | **3.4배 상승!** |
| **상태** | ✅ 성공 | ✅ **압도적 성공** | - |

**데이터는 10배 늘어났는데, 시간은 3배도 안 걸렸습니다.** 이것이 바로 인프라를 증설하지 않고도 성능을 뽑아내는 소프트웨어 최적화의 힘입니다.

---

## 🔍 Deep Dive: 왜 이렇게 빨라진 걸까?

### 1. 셔플(Shuffle)의 공포를 극복하다
1억 건 급에서는 셔플 데이터가 디스크로 유출(Spill)되기 시작하면 지옥문이 열립니다. 저희는 `shuffle.partitions`를 조절하고 메모리를 넉넉히 주어 대부분의 연산을 메모리 내에서 끝냈습니다.

### 2. Delta Lake의 "데이터 스키핑"  skipping
Z-Ordering을 적용한 후, Spark은 더 이상 1억 건 전체를 보지 않습니다. 인덱싱된 메타데이터를 보고 필요한 데이터가 들어있는 파일만 콕 집어서 읽습니다.

### 3. 파티션 프루닝(Pruning)
`year/month` 파티셔닝 덕분에 "2023년 12월 데이터만 가져와"라는 쿼리가 들어오면, 나머지 11개월분 데이터는 아예 거들떠보지도 않습니다.

---

## 💡 이번 도전을 통해 느낀 점

### 1. "장비 탓" 하기 전에 "코드 탓"부터!
로컬 맥북에서도 1억 건을 12분 만에 처리할 수 있다는 점은 놀라운 발견이었습니다. 클라우드 비용을 아끼는 최고의 방법은 역시 엔지니어링 역량이라는 걸 다시 한번 깨달았습니다.

### 2. 스케일의 임계점
어떤 기술이든 임계점이 존재합니다. 이번 100M 챌린지는 그 임계점을 최적화로 어떻게 밀어낼 수 있는지 보여주는 완벽한 사례였습니다.

---

## 📝 마무리하며

이제 우리 엔진은 **"1억 건 정도는 껌"**인 상태가 되었습니다. 로컬 PC에서 이 정도 성능을 냈다면, 실제 클러스터 환경에서는 상상 그 이상의 퍼포먼스를 보여줄 것입니다.

**"데이터가 많아질수록 느려지는 게 아니라, 더 똑똑하게 처리하는 것."** 그것이 데이터 엔지니어의 자존심이죠!

---

**Tags**: `#Spark` `#DeltaLake` `#ZOrder` `#Partitioning` `#BigData` `#Optimization` `#M2Pro`

**다음 단계**: 실시간 스트리밍 처리와 Grafana 대시보드 시각화 가즈아! 📊✨
