# Personal Finance Data Platform - Apache Spark ë°°ì¹˜ ì²˜ë¦¬ í”„ë¡œì íŠ¸

> **í•™ìŠµ ëª©í‘œ**: Apache Sparkë¥¼ í™œìš©í•œ ë°°ì¹˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•, Medallion Architecture ì‹¤ìŠµ, Star Schema ê¸°ë°˜ ë°ì´í„° ë§ˆíŠ¸ ì„¤ê³„

---

## 1. ê¸°ìˆ  ìŠ¤íƒ ë° ì•„í‚¤í…ì²˜ ì„ ì • ê·¼ê±°

### 1.1 Apache Spark ì„ íƒ ì´ìœ  (ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê´€ì )

| ìš”êµ¬ì‚¬í•­ | Spark ì í•©ì„± | ì´ìœ  |
|---------|------------|------|
| ë°°ì¹˜ ì²˜ë¦¬ | âœ… ìµœì  | ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ì˜ ì‚¬ì‹¤ìƒ í‘œì¤€ |
| í•™ìŠµ ëª©ì  | âœ… ìµœì  | í˜„ì—…ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë¶„ì‚° ì²˜ë¦¬ í”„ë ˆì„ì›Œí¬ |
| ë°ì´í„° ë³€í™˜ | âœ… ìµœì  | DataFrame APIë¡œ ë³µì¡í•œ ETL ë¡œì§ êµ¬í˜„ ìš©ì´ |
| í™•ì¥ì„± | âœ… ìµœì  | ë¡œì»¬ â†’ í´ëŸ¬ìŠ¤í„° ì „í™˜ ê°€ëŠ¥ |

### 1.2 ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ (2024-2025)

```mermaid
graph TB
    A[Data Generation Script] --> B[Raw Data Lake - Parquet]
    B --> C[Apache Spark Batch Jobs]
    C --> D[Bronze Layer - Delta Lake]
    D --> E[Silver Layer - Delta Lake]
    E --> F[Gold Layer - Star Schema]
    F --> G[Apache Superset Dashboard]
    
    style D fill:#cd7f32
    style E fill:#c0c0c0
    style F fill:#ffd700
```

**í•µì‹¬ ê¸°ìˆ **:
- **ë°ì´í„° ì²˜ë¦¬**: Apache Spark 3.5+ (PySpark)
- **ìŠ¤í† ë¦¬ì§€ í¬ë§·**: Delta Lake (ACID íŠ¸ëœì­ì…˜, íƒ€ì„ íŠ¸ë˜ë¸”)
- **ë°ì´í„° ìƒì„±**: Faker ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ìŠ¤í¬ë¦½íŠ¸
- **ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: Apache Airflow (ì„ íƒ)
- **ì‹œê°í™”**: Apache Superset ë˜ëŠ” Metabase
- **ë©”íƒ€ë°ì´í„°**: Hive Metastore (ì„ íƒ)

### 1.3 Medallion Architecture - ì—¬ì „íˆ íŠ¸ë Œë””í•œê°€?

**ë‹µ: YES! 2024ë…„ í˜„ì¬ë„ ì—…ê³„ í‘œì¤€** âœ…

- **Databricks**: ê³µì‹ ê¶Œì¥ ì•„í‚¤í…ì²˜
- **Snowflake**: Multi-tier ì•„í‚¤í…ì²˜ë¡œ ë™ì¼ ê°œë… ì‚¬ìš©
- **AWS Lake Formation**: Bronze/Silver/Gold ë ˆì´ì–´ ì§€ì›
- **Azure Synapse**: Medallion íŒ¨í„´ ê³µì‹ ë¬¸ì„œí™”

**ìµœì‹  íŠ¸ë Œë“œ ì¶”ê°€**:
- Delta Lakeë¡œ ê° ë ˆì´ì–´ êµ¬í˜„ (ACID, Schema Evolution)
- Data Quality ì²´í¬ ìë™í™” (Great Expectations)
- Lakehouse ì•„í‚¤í…ì²˜ (Data Warehouse + Data Lake)

---

## 2. ë°ì´í„° ìƒì„± ì „ëµ

### 2.1 CSV vs ìŠ¤í¬ë¦½íŠ¸ ê¸°ë°˜ ìƒì„±

| ë°©ì‹ | ì¥ì  | ë‹¨ì  | ì„ íƒ |
|-----|------|------|------|
| CSV íŒŒì¼ | ê°„ë‹¨, ì‹¤ì œ ë°ì´í„° í˜•íƒœ | í™•ì¥ ì–´ë ¤ì›€, ìˆ˜ë™ ì‘ì—… | âŒ |
| Python ìŠ¤í¬ë¦½íŠ¸ | ìë™í™”, ëŒ€ìš©ëŸ‰ ìƒì„± ê°€ëŠ¥, ì¬í˜„ ê°€ëŠ¥ | ì´ˆê¸° êµ¬í˜„ í•„ìš” | âœ… **ì¶”ì²œ** |

### 2.2 ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì„¤ê³„

```python
# data_generator/transaction_generator.py
from faker import Faker
import pandas as pd
from datetime import datetime, timedelta
import random

class TransactionGenerator:
    def __init__(self, seed=42):
        self.fake = Faker('ko_KR')
        Faker.seed(seed)
        random.seed(seed)
    
    def generate_card_transactions(self, num_records=100000, 
                                   start_date='2023-01-01', 
                                   end_date='2024-12-31'):
        """ì¹´ë“œ ê±°ë˜ ë°ì´í„° ìƒì„± (10ë§Œê±´ ê¸°ë³¸)"""
        
        merchants = self._load_merchant_catalog()
        
        data = []
        for _ in range(num_records):
            merchant = random.choice(merchants)
            txn_date = self.fake.date_between(
                start_date=datetime.strptime(start_date, '%Y-%m-%d'),
                end_date=datetime.strptime(end_date, '%Y-%m-%d')
            )
            
            data.append({
                'transaction_id': self.fake.uuid4(),
                'transaction_date': txn_date,
                'merchant_name': merchant['name'],
                'merchant_category': merchant['category'],  # ì •ë‹µ ë ˆì´ë¸”
                'amount': random.randint(1000, 500000),
                'card_number': f"****{random.randint(1000, 9999)}",
                'created_at': datetime.now()
            })
        
        return pd.DataFrame(data)
    
    def _load_merchant_catalog(self):
        """ì‹¤ì œ ìƒí˜¸ëª… ì¹´íƒˆë¡œê·¸"""
        return [
            {'name': 'ìŠ¤íƒ€ë²…ìŠ¤ ê°•ë‚¨ì ', 'category': 'ì‹ë¹„-ì¹´í˜'},
            {'name': 'ì´ë§ˆíŠ¸ ì›”ê³„ì ', 'category': 'ì‹ë¹„-ë§ˆíŠ¸'},
            {'name': 'CGV ìš©ì‚°', 'category': 'ë¬¸í™”-ì˜í™”'},
            # ... 100ê°œ ì´ìƒì˜ ìƒí˜¸ëª…
        ]
```

**ìƒì„± ë°ì´í„° ê·œëª¨**:
- ì¹´ë“œ ê±°ë˜: 100,000ê±´ (2ë…„ì¹˜)
- ì€í–‰ ì…ì¶œê¸ˆ: 50,000ê±´
- ì£¼ì‹ ê±°ë˜: 10,000ê±´
- ì ê¸ˆ: 100ê±´

---

## 3. Star Schema ê¸°ë°˜ ë°ì´í„° ë§ˆíŠ¸ ì„¤ê³„

### 3.1 Star Schema ê°œë…

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ dim_date    â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ dim_merchantâ”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  fact_transactions      â”‚ â—„â”€â”€â”€ Fact Table (ì¤‘ì‹¬)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ dim_categoryâ”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
        â”‚ dim_card    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Fact Table ì„¤ê³„

#### `fact_transactions` (ì‚¬ì‹¤ í…Œì´ë¸”)

```sql
CREATE TABLE gold.fact_transactions (
    -- Surrogate Key
    transaction_key BIGINT PRIMARY KEY,
    
    -- Foreign Keys (Dimension ì—°ê²°)
    date_key INT NOT NULL,
    merchant_key INT NOT NULL,
    category_key INT NOT NULL,
    card_key INT NOT NULL,
    
    -- Degenerate Dimensions (Factì—ë§Œ ì¡´ì¬)
    transaction_id STRING,
    
    -- Measures (ì§‘ê³„ ëŒ€ìƒ)
    amount DECIMAL(15,2) NOT NULL,
    quantity INT DEFAULT 1,
    
    -- Metadata
    created_at TIMESTAMP,
    updated_at TIMESTAMP
)
USING DELTA
PARTITIONED BY (date_key);
```

### 3.3 Dimension Tables ì„¤ê³„

#### `dim_date` (ë‚ ì§œ ì°¨ì›)

```sql
CREATE TABLE gold.dim_date (
    date_key INT PRIMARY KEY,  -- 20240101
    full_date DATE NOT NULL,
    year INT,
    quarter INT,
    month INT,
    month_name STRING,
    day_of_month INT,
    day_of_week INT,
    day_name STRING,
    is_weekend BOOLEAN,
    is_holiday BOOLEAN,
    fiscal_year INT,
    fiscal_quarter INT
)
USING DELTA;
```

#### `dim_merchant` (ìƒì  ì°¨ì›)

```sql
CREATE TABLE gold.dim_merchant (
    merchant_key INT PRIMARY KEY,
    merchant_id STRING,
    merchant_name STRING,
    normalized_name STRING,
    merchant_type STRING,
    location STRING,
    -- SCD Type 2 (ì²œì²œíˆ ë³€í•˜ëŠ” ì°¨ì›)
    effective_date DATE,
    expiration_date DATE,
    is_current BOOLEAN
)
USING DELTA;
```

#### `dim_category` (ì¹´í…Œê³ ë¦¬ ì°¨ì›)

```sql
CREATE TABLE gold.dim_category (
    category_key INT PRIMARY KEY,
    category_id STRING,
    category_name STRING,
    subcategory_name STRING,
    category_group STRING,
    -- Hierarchy
    level_1 STRING,  -- ëŒ€ë¶„ë¥˜: ì‹ë¹„
    level_2 STRING,  -- ì¤‘ë¶„ë¥˜: ì™¸ì‹
    level_3 STRING   -- ì†Œë¶„ë¥˜: ì¹´í˜
)
USING DELTA;
```

#### `dim_card` (ì¹´ë“œ ì°¨ì›)

```sql
CREATE TABLE gold.dim_card (
    card_key INT PRIMARY KEY,
    card_number_masked STRING,
    card_type STRING,  -- ì‹ ìš©/ì²´í¬
    card_company STRING,
    issue_date DATE
)
USING DELTA;
```

### 3.4 Star Schemaì˜ ì¥ì 

| ì¥ì  | ì„¤ëª… |
|-----|------|
| **ì¿¼ë¦¬ ì„±ëŠ¥** | ì¡°ì¸ ìµœì†Œí™” (Fact â†” Dimë§Œ) |
| **ì´í•´ ìš©ì´** | ë¹„ì¦ˆë‹ˆìŠ¤ ì‚¬ìš©ìë„ ì´í•´ ê°€ëŠ¥ |
| **ì§‘ê³„ ìµœì í™”** | BI ë„êµ¬ ìµœì í™” |
| **í™•ì¥ì„±** | Dimension ë…ë¦½ì  í™•ì¥ |

---

## 4. Medallion Architecture êµ¬í˜„

### 4.1 Bronze Layer (Raw Data)

**ëª©ì **: ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì €ì¥ (Immutable)

```python
# jobs/bronze_ingestion.py
from pyspark.sql import SparkSession

def ingest_to_bronze(spark, source_path, table_name):
    """
    Parquet íŒŒì¼ì„ Delta Lake Bronzeë¡œ ì ì¬
    """
    df = spark.read.parquet(source_path)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    df_with_meta = df \
        .withColumn("ingestion_timestamp", current_timestamp()) \
        .withColumn("source_file", input_file_name())
    
    # Delta Lakeì— ì €ì¥ (Append Only)
    df_with_meta.write \
        .format("delta") \
        .mode("append") \
        .option("mergeSchema", "true") \
        .save(f"/data/bronze/{table_name}")
```

**ìŠ¤í‚¤ë§ˆ**:
```python
bronze_card_schema = StructType([
    StructField("transaction_id", StringType(), False),
    StructField("transaction_date", DateType(), False),
    StructField("merchant_name", StringType(), False),
    StructField("amount", DecimalType(15,2), False),
    StructField("card_number", StringType(), True),
    StructField("ingestion_timestamp", TimestampType(), False),
    StructField("source_file", StringType(), True)
])
```

### 4.2 Silver Layer (Cleaned & Validated)

**ëª©ì **: ë°ì´í„° í’ˆì§ˆ ê²€ì¦, ì •ì œ, ì¤‘ë³µ ì œê±°

```python
# jobs/silver_transformation.py
from pyspark.sql.functions import *
from delta.tables import DeltaTable

def transform_to_silver(spark, bronze_path, silver_path):
    """
    Bronze â†’ Silver ë³€í™˜
    - ì¤‘ë³µ ì œê±°
    - ë°ì´í„° ê²€ì¦
    - ì •ê·œí™”
    """
    df_bronze = spark.read.format("delta").load(bronze_path)
    
    # 1. ì¤‘ë³µ ì œê±° (transaction_id ê¸°ì¤€)
    df_dedup = df_bronze.dropDuplicates(["transaction_id"])
    
    # 2. ë°ì´í„° ê²€ì¦
    df_validated = df_dedup.filter(
        (col("amount") > 0) &
        (col("transaction_date").isNotNull()) &
        (col("merchant_name").isNotNull())
    )
    
    # 3. ìƒí˜¸ëª… ì •ê·œí™”
    df_normalized = df_validated.withColumn(
        "normalized_merchant",
        normalize_merchant_udf(col("merchant_name"))
    )
    
    # 4. ê±°ë˜ ìœ í˜• ë¶„ë¥˜
    df_typed = df_normalized.withColumn(
        "transaction_type",
        when(col("amount") > 0, "INCOME")
        .otherwise("EXPENSE")
    )
    
    # 5. Delta Lake Merge (Upsert)
    if DeltaTable.isDeltaTable(spark, silver_path):
        delta_table = DeltaTable.forPath(spark, silver_path)
        delta_table.alias("target").merge(
            df_typed.alias("source"),
            "target.transaction_id = source.transaction_id"
        ).whenMatchedUpdateAll() \
         .whenNotMatchedInsertAll() \
         .execute()
    else:
        df_typed.write.format("delta").save(silver_path)
```

**Data Quality ì²´í¬**:
```python
# utils/data_quality.py
from great_expectations.dataset import SparkDFDataset

def validate_silver_data(df):
    """Great Expectationsë¡œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    ge_df = SparkDFDataset(df)
    
    # ê²€ì¦ ê·œì¹™
    ge_df.expect_column_values_to_not_be_null("transaction_id")
    ge_df.expect_column_values_to_be_unique("transaction_id")
    ge_df.expect_column_values_to_be_between("amount", 0, 10000000)
    ge_df.expect_column_values_to_be_of_type("transaction_date", "DateType")
    
    results = ge_df.validate()
    return results.success
```

### 4.3 Gold Layer (Star Schema Data Mart)

**ëª©ì **: ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ìœ„í•œ ìµœì í™”ëœ ë°ì´í„° ë§ˆíŠ¸

```python
# jobs/gold_star_schema.py

def build_fact_table(spark, silver_path):
    """
    Silver â†’ Gold Fact Table ìƒì„±
    """
    df_silver = spark.read.format("delta").load(silver_path)
    
    # Dimension í…Œì´ë¸” ì¡°íšŒ
    dim_date = spark.read.format("delta").load("/data/gold/dim_date")
    dim_merchant = spark.read.format("delta").load("/data/gold/dim_merchant")
    dim_category = spark.read.format("delta").load("/data/gold/dim_category")
    dim_card = spark.read.format("delta").load("/data/gold/dim_card")
    
    # Fact í…Œì´ë¸” ìƒì„± (Surrogate Key ì¡°ì¸)
    fact_df = df_silver \
        .join(dim_date, 
              to_date(df_silver.transaction_date) == dim_date.full_date) \
        .join(dim_merchant.filter(col("is_current") == True),
              df_silver.normalized_merchant == dim_merchant.normalized_name) \
        .join(dim_category,
              df_silver.category == dim_category.category_name) \
        .join(dim_card,
              df_silver.card_number == dim_card.card_number_masked) \
        .select(
            monotonically_increasing_id().alias("transaction_key"),
            dim_date.date_key,
            dim_merchant.merchant_key,
            dim_category.category_key,
            dim_card.card_key,
            df_silver.transaction_id,
            df_silver.amount,
            current_timestamp().alias("created_at")
        )
    
    # Delta Lakeì— ì €ì¥ (Partitioned by date_key)
    fact_df.write \
        .format("delta") \
        .mode("overwrite") \
        .partitionBy("date_key") \
        .save("/data/gold/fact_transactions")
```

**Dimension í…Œì´ë¸” ìƒì„±**:
```python
def build_dim_date(spark, start_date, end_date):
    """ë‚ ì§œ ì°¨ì› í…Œì´ë¸” ìƒì„±"""
    from datetime import datetime, timedelta
    
    dates = []
    current = datetime.strptime(start_date, '%Y-%m-%d')
    end = datetime.strptime(end_date, '%Y-%m-%d')
    
    while current <= end:
        dates.append({
            'date_key': int(current.strftime('%Y%m%d')),
            'full_date': current.date(),
            'year': current.year,
            'quarter': (current.month - 1) // 3 + 1,
            'month': current.month,
            'month_name': current.strftime('%B'),
            'day_of_month': current.day,
            'day_of_week': current.weekday(),
            'day_name': current.strftime('%A'),
            'is_weekend': current.weekday() >= 5,
            'is_holiday': check_holiday(current)  # ê³µíœ´ì¼ ì²´í¬ ë¡œì§
        })
        current += timedelta(days=1)
    
    df = spark.createDataFrame(dates)
    df.write.format("delta").mode("overwrite").save("/data/gold/dim_date")
```

---

## 5. í”„ë¡œì íŠ¸ êµ¬ì¡°

```
finance-spark-platform/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # ìƒì„±ëœ Parquet íŒŒì¼
â”‚   â”œâ”€â”€ bronze/                    # Delta Lake Bronze
â”‚   â”œâ”€â”€ silver/                    # Delta Lake Silver
â”‚   â””â”€â”€ gold/                      # Delta Lake Gold (Star Schema)
â”‚
â”œâ”€â”€ data_generator/                # ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transaction_generator.py
â”‚   â”œâ”€â”€ merchant_catalog.yaml
â”‚   â””â”€â”€ generate_all.py
â”‚
â”œâ”€â”€ jobs/                          # Spark Job ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ bronze_ingestion.py
â”‚   â”œâ”€â”€ silver_transformation.py
â”‚   â”œâ”€â”€ gold_star_schema.py
â”‚   â””â”€â”€ categorization_job.py
â”‚
â”œâ”€â”€ schemas/                       # ìŠ¤í‚¤ë§ˆ ì •ì˜
â”‚   â”œâ”€â”€ bronze_schemas.py
â”‚   â”œâ”€â”€ silver_schemas.py
â”‚   â””â”€â”€ gold_schemas.py
â”‚
â”œâ”€â”€ utils/                         # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ spark_session.py
â”‚   â”œâ”€â”€ data_quality.py
â”‚   â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ airflow/                       # Airflow DAGs (ì„ íƒ)
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ daily_etl_pipeline.py
â”‚
â”œâ”€â”€ sql/                           # SQL ì¿¼ë¦¬
â”‚   â”œâ”€â”€ create_dimensions.sql
â”‚   â””â”€â”€ analytics_queries.sql
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_etl_development.ipynb
â”‚   â””â”€â”€ 03_analytics.ipynb
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â””â”€â”€ test_data_quality.py
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ spark_config.yaml
â”‚   â””â”€â”€ pipeline_config.yaml
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ roadmap.md
â”‚
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## 6. í•µì‹¬ ì½”ë“œ êµ¬í˜„

### 6.1 Spark Session ì„¤ì •

```python
# utils/spark_session.py
from pyspark.sql import SparkSession

def create_spark_session(app_name="FinanceDataPlatform"):
    """
    Delta Lake ì§€ì› Spark Session ìƒì„±
    """
    spark = SparkSession.builder \
        .appName(app_name) \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.databricks.delta.retentionDurationCheck.enabled", "false") \
        .master("local[*]") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark
```

### 6.2 ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

```python
# jobs/run_pipeline.py
from utils.spark_session import create_spark_session
from jobs.bronze_ingestion import ingest_to_bronze
from jobs.silver_transformation import transform_to_silver
from jobs.gold_star_schema import build_fact_table, build_dimensions

def run_full_pipeline():
    """ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    spark = create_spark_session()
    
    try:
        # Step 1: Bronze Ingestion
        print("ğŸ”¶ Bronze Layer ì ì¬ ì‹œì‘...")
        ingest_to_bronze(spark, "/data/raw/card_transactions.parquet", "card_transactions")
        
        # Step 2: Silver Transformation
        print("ğŸ”· Silver Layer ë³€í™˜ ì‹œì‘...")
        transform_to_silver(spark, "/data/bronze/card_transactions", "/data/silver/transactions")
        
        # Step 3: Build Dimensions
        print("ğŸŒŸ Dimension í…Œì´ë¸” ìƒì„±...")
        build_dimensions(spark)
        
        # Step 4: Build Fact Table
        print("ğŸŒŸ Fact í…Œì´ë¸” ìƒì„±...")
        build_fact_table(spark, "/data/silver/transactions")
        
        print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        raise
    finally:
        spark.stop()

if __name__ == "__main__":
    run_full_pipeline()
```

### 6.3 ë¶„ì„ ì¿¼ë¦¬ ì˜ˆì‹œ

```python
# analytics/monthly_report.py

def generate_monthly_report(spark, year_month):
    """
    Star Schemaë¥¼ í™œìš©í•œ ì›”ë³„ ë¦¬í¬íŠ¸
    """
    query = f"""
    SELECT 
        d.month_name,
        c.category_name,
        c.subcategory_name,
        COUNT(*) as transaction_count,
        SUM(f.amount) as total_amount,
        AVG(f.amount) as avg_amount,
        MAX(f.amount) as max_amount
    FROM gold.fact_transactions f
    JOIN gold.dim_date d ON f.date_key = d.date_key
    JOIN gold.dim_category c ON f.category_key = c.category_key
    WHERE d.year = {year_month[:4]}
      AND d.month = {int(year_month[5:])}
    GROUP BY d.month_name, c.category_name, c.subcategory_name
    ORDER BY total_amount DESC
    """
    
    return spark.sql(query)
```

---

## 7. 2ì£¼ ê°œë°œ íƒ€ì„ë¼ì¸

### Week 1: ë°ì´í„° ìƒì„± ë° Bronze/Silver êµ¬ì¶• âœ… COMPLETED

#### Day 1-2: í™˜ê²½ ì„¤ì • ë° ë°ì´í„° ìƒì„±
- [x] Spark 3.5 + Delta Lake ì„¤ì¹˜
- [x] ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (Faker ê¸°ë°˜)
- [x] 10ë§Œê±´ ì¹´ë“œ ê±°ë˜ ë°ì´í„° ìƒì„± (Parquet)
- [x] ìƒí˜¸ëª… ì¹´íƒˆë¡œê·¸ 40ê°œ ì‘ì„±

#### Day 3-4: Bronze Layer
- [x] Bronze ìŠ¤í‚¤ë§ˆ ì •ì˜
- [x] Parquet â†’ Delta Lake ì ì¬ Job
- [x] ë©”íƒ€ë°ì´í„° ì¶”ê°€ ë¡œì§
- [x] Spark-Parquet í˜¸í™˜ì„± ìˆ˜ì •

#### Day 5-7: Silver Layer
- [x] ë°ì´í„° ì •ì œ ë¡œì§ (ì¤‘ë³µ ì œê±°, ê²€ì¦)
- [x] ìƒí˜¸ëª… ì •ê·œí™” UDF
- [x] Delta Lake Merge (Upsert) êµ¬í˜„
- [x] ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë¡œì§

### Week 2: Gold Layer (Star Schema) ë° ë¶„ì„ ğŸ”„ IN PROGRESS

#### Day 8-10: Dimension Tables
- [x] `dim_date` ìƒì„± (2ë…„ì¹˜, 731ì¼)
- [x] `dim_merchant` ìƒì„± (41ê°œ ìƒì )
- [x] `dim_category` ìƒì„± (18ê°œ ì¹´í…Œê³ ë¦¬, ê³„ì¸µ êµ¬ì¡°)
- [ ] `dim_card` ìƒì„± (ì„ íƒì‚¬í•­)

#### Day 11-12: Fact Table
- [x] `fact_transactions` ìƒì„±
- [x] Surrogate Key ì¡°ì¸ ë¡œì§
- [x] Partitioning ìµœì í™” (date_key)
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬

#### Day 13-14: ë¶„ì„ ë° ì‹œê°í™” â³ NEXT
- [ ] ì›”ë³„/ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„ ì¿¼ë¦¬
- [ ] Jupyter Notebook ë¶„ì„
- [ ] ë°ì´í„° ì‹œê°í™” (matplotlib, seaborn)
- [ ] ë¬¸ì„œí™” ë° ì •ë¦¬

---

## 8. ë¶„ì„ ì¿¼ë¦¬ ì˜ˆì‹œ (Star Schema í™œìš©)

### 8.1 ì›”ë³„ ì¹´í…Œê³ ë¦¬ ì§€ì¶œ ë¶„ì„

```sql
SELECT 
    d.year,
    d.month_name,
    c.level_1 as main_category,
    SUM(f.amount) as total_spent,
    COUNT(*) as transaction_count
FROM gold.fact_transactions f
JOIN gold.dim_date d ON f.date_key = d.date_key
JOIN gold.dim_category c ON f.category_key = c.category_key
WHERE d.year = 2024
GROUP BY d.year, d.month_name, c.level_1
ORDER BY d.month, total_spent DESC;
```

### 8.2 ì£¼ë§ vs í‰ì¼ ì†Œë¹„ íŒ¨í„´

```sql
SELECT 
    d.is_weekend,
    c.category_name,
    AVG(f.amount) as avg_amount,
    COUNT(*) as txn_count
FROM gold.fact_transactions f
JOIN gold.dim_date d ON f.date_key = d.date_key
JOIN gold.dim_category c ON f.category_key = c.category_key
GROUP BY d.is_weekend, c.category_name
ORDER BY avg_amount DESC;
```

### 8.3 Top 10 ìƒì  ë¶„ì„

```sql
SELECT 
    m.merchant_name,
    c.category_name,
    SUM(f.amount) as total_spent,
    COUNT(*) as visit_count
FROM gold.fact_transactions f
JOIN gold.dim_merchant m ON f.merchant_key = m.merchant_key
JOIN gold.dim_category c ON f.category_key = c.category_key
WHERE m.is_current = true
GROUP BY m.merchant_name, c.category_name
ORDER BY total_spent DESC
LIMIT 10;
```

---

## 9. Delta Lake ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

### 9.1 Time Travel (ë°ì´í„° ë²„ì „ ê´€ë¦¬)

```python
# íŠ¹ì • ì‹œì  ë°ì´í„° ì¡°íšŒ
df_yesterday = spark.read \
    .format("delta") \
    .option("versionAsOf", 1) \
    .load("/data/silver/transactions")

# íŠ¹ì • íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° ì¡°íšŒ
df_snapshot = spark.read \
    .format("delta") \
    .option("timestampAsOf", "2024-01-01") \
    .load("/data/silver/transactions")
```

### 9.2 VACUUM (ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬)

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forPath(spark, "/data/silver/transactions")
delta_table.vacuum(168)  # 7ì¼ ì´ìƒ ëœ íŒŒì¼ ì‚­ì œ
```

### 9.3 OPTIMIZE (íŒŒì¼ ì••ì¶•)

```python
# ì‘ì€ íŒŒì¼ë“¤ì„ í° íŒŒì¼ë¡œ ë³‘í•©
spark.sql("OPTIMIZE delta.`/data/gold/fact_transactions` ZORDER BY (date_key)")
```

---

## 10. ì‹¤í–‰ ê°€ì´ë“œ

### 10.1 ì´ˆê¸° ì„¤ì • (uv ì‚¬ìš©)

```bash
# 1. uv ì„¤ì¹˜ (macOS)
curl -LsSf https://astral.sh/uv/install.sh | sh

# 2. í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
uv init finance-spark-platform
cd finance-spark-platform

# 3. Python 3.11 ì„¤ì¹˜ ë° ê°€ìƒí™˜ê²½ ìƒì„±
uv python install 3.11
uv venv

# 4. ê°€ìƒí™˜ê²½ í™œì„±í™”
source .venv/bin/activate

# 5. íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv pip install pyspark==3.5.0 delta-spark==3.0.0 faker==22.0.0 pandas pyarrow great-expectations

# 6. ë°ì´í„° ìƒì„±
uv run python data_generator/generate_all.py --records 100000

# 7. Spark íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
uv run python jobs/run_pipeline.py
```

### 10.2 pyproject.toml (uv ë°©ì‹)

```toml
[project]
name = "finance-spark-platform"
version = "0.1.0"
description = "Personal Finance Data Platform with Apache Spark"
requires-python = ">=3.11"
dependencies = [
    "pyspark==3.5.0",
    "delta-spark==3.0.0",
    "faker==22.0.0",
    "pandas>=2.1.4",
    "pyarrow>=14.0.1",
    "great-expectations>=0.18.8",
    "pyyaml>=6.0.1",
]

[project.optional-dependencies]
dev = [
    "jupyter>=1.0.0",
    "matplotlib>=3.8.2",
    "seaborn>=0.13.0",
    "pytest>=7.4.0",
    "ruff>=0.1.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
line-length = 100
target-version = "py311"
```

### 10.3 Jupyter Notebook ì‹¤í–‰

```bash
uv pip install jupyter matplotlib seaborn
uv run jupyter notebook notebooks/02_etl_development.ipynb
```

---

## 11. í•™ìŠµ í¬ì¸íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Apache Spark
- [ ] DataFrame API í™œìš©
- [ ] UDF (User Defined Function) ì‘ì„±
- [ ] Partitioning ì „ëµ
- [ ] Broadcast Join vs Shuffle Join
- [ ] Adaptive Query Execution

### Delta Lake
- [ ] ACID íŠ¸ëœì­ì…˜
- [ ] Time Travel
- [ ] Schema Evolution
- [ ] MERGE (Upsert) ì—°ì‚°
- [ ] OPTIMIZE & VACUUM

### Data Modeling
- [ ] Medallion Architecture êµ¬í˜„
- [ ] Star Schema ì„¤ê³„
- [ ] Slowly Changing Dimensions (SCD Type 2)
- [ ] Surrogate Key ê´€ë¦¬
- [ ] Fact vs Dimension êµ¬ë¶„

### Data Quality
- [ ] Great Expectations ê²€ì¦
- [ ] ì¤‘ë³µ ì œê±° ì „ëµ
- [ ] Null ì²˜ë¦¬
- [ ] ë°ì´í„° íƒ€ì… ê²€ì¦

---

## 12. ë‹¤ìŒ ë‹¨ê³„ (í™•ì¥ ì•„ì´ë””ì–´)

1. **Airflow ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: DAGë¡œ ì¼ì¼ ë°°ì¹˜ ìë™í™”
2. **Incremental Load**: ì „ì²´ ì¬ì²˜ë¦¬ â†’ ì¦ë¶„ ì²˜ë¦¬
3. **Slowly Changing Dimensions**: SCD Type 2 êµ¬í˜„
4. **Data Lineage**: OpenLineageë¡œ ë°ì´í„° ê³„ë³´ ì¶”ì 
5. **ML Pipeline**: Spark MLlibë¡œ ì§€ì¶œ ì˜ˆì¸¡ ëª¨ë¸

---

ì´ ë¡œë“œë§µì€ **í˜„ì—… ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ í‘œì¤€**ì„ ë”°ë¥´ë©°, Spark í•™ìŠµê³¼ ì‹¤ë¬´ ì—­ëŸ‰ì„ ë™ì‹œì— í‚¤ìš¸ ìˆ˜ ìˆëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤! ğŸš€
