# 1억 건의 금융 데이터를 24초 만에 처리하는 방법: Spark에서 사건의 지평선까지

안녕하세요, 데이터 플랫폼 팀입니다. 오늘은 로컬 환경에서 **1억 건(100,000,000개)**의 금융 데이터를 정제하고 분석용 데이터 마트(Gold Layer)로 가공하는 과정에서 겪은 성능 최적화 여정을 공유하고자 합니다.

11분 26초에서 시작해 **단 24.6초** 만에 마침표를 찍기까지, 저희가 만난 기술적 장벽과 그것을 넘어선 과정을 데이터와 함께 담았습니다.

---

## 1. 문제 상황: "로컬에서 1억 건, 가능할까?"

금융 데이터 시스템에서 1억 건은 '작지만 큰' 숫자입니다. 분산 클러스터를 쓰기엔 아쉽고, 로컬 머신에서 처리하기엔 무거운 규모죠. 저희의 목표는 명확했습니다.

> **"로컬 머신(M2 Pro, 32GB RAM)에서 1억 건의 거래 데이터를 1분 이내에 ETL(Extract, Transform, Load)한다."**

초기 상태의 Spark 기반 Medallion 파이프라인(Bronze-Silver-Gold)의 성적표는 **11분 26초(686.5초)**였습니다. 목표인 60초까지는 약 11배의 개선이 필요한 상황이었습니다.

## 2. 해결 과정: 4단계의 도약

우리는 점진적으로 병목을 찾아내며 아키텍처를 뒤집었습니다.

### [Phase 1] Spark의 잠재력 끌어올리기 (686s → 200s)
가장 먼저 Spark 내부의 비효율을 제거했습니다. 
- **Z-Order & Hierarchical Partitioning**: 날짜 기반 파티셔닝과 Delta Lake의 Z-Order 최적화를 통해 디스크 I/O를 획기적으로 줄였습니다.
- **Supersonic One-Pass DAG**: Bronze, Silver 레이어를 매번 디스크에 쓰는 대신, 메모리 내에서 하나의 거대한 DAG로 처리하여 I/O 발생 지점을 단 한 곳(Gold)으로 압축했습니다.

결과적으로 약 **3.4배**의 성능 향상을 이뤄냈지만, 여전히 3.3분(200초)이라는 벽에 부딪혔습니다.

### [Phase 2] 관점의 전환: "JVM이라는 장벽" (200s → 86s)
Spark를 집요하게 튜닝하던 중, 우리는 충격적인 사실을 발견했습니다. 1억 건의 연산 중 **약 30~40초가 Spark의 JVM 시작과 Python-Scala 통신 준비**에만 쓰이고 있었습니다.

로컬 환경에서의 1분 이내 도달을 위해, 우리는 과감히 **Native C++ 엔진인 DuckDB**로 엔진을 교체하는 'Quantum' 모드를 도입했습니다. JVM 오버헤드가 사라지자마자 기록은 **86.5초**로 단축되었습니다.

### [Phase 3] 알고리즘의 최적화: Hash vs Window (86s → 35s)
86초를 넘어서기 위해 연산 알고리즘을 분석했습니다. 기존의 중복 제거(Deduplication)에 쓰이던 `Window Function`은 전체 데이터를 정렬해야 하는 고비용 연산이었습니다. 

이를 **Hash-based Grouping**(`GROUP BY` + `any_value`)으로 전환했습니다. 전역 정렬 부하가 사라지자 처리 속도는 초당 **280만 건**으로 치솟으며 **35.89초**를 기록, 드디어 60초의 벽을 깨뜨렸습니다.

### [Phase 4] 사건의 지평선: 물리적 한계에 도전 (35s → 24s)
우리는 멈추지 않고 하드웨어의 최대 성능을 측정했습니다. 이 장비에서 1억 건을 단순히 읽고 쓰는 데만 **8.1초**가 걸립니다. 연산에 쓸 수 있는 물리적 시간은 단 20여 초뿐이었습니다.

우리는 CPU가 압축(Snappy) 연산에 쓰는 시간조차 아깝다고 판단, **Zero-Compression & Zero-Copy Projection** 전략을 선택했습니다. CPU를 오직 데이터 이동과 조인에만 쏟아부은 결과, 최종 **24.65초**라는 경이로운 기록에 도달했습니다.

## 3. 핵심 인사이트: 우리가 배운 것들

### 1) 엔진의 선택은 환경에 따라 달라야 합니다
분산 환경에서는 Spark가 정답일 수 있지만, 단일 노드의 극한 성능이 필요한 경우 Native 엔진(DuckDB, Polars)의 효율은 압도적이었습니다. 초당 405만 건의 처리량은 도합 27배의 개선을 만들어냈습니다.

### 2) 알고리즘이 I/O보다 중요할 때가 있습니다
1억 건 규모에서는 디스크 I/O 못지않게 CPU의 정렬 오버헤드가 큽니다. Hash 기반 연산으로의 전환만으로 50초 이상의 시간을 단축할 수 있었습니다.

### 3) 하드웨어의 밑바닥(Floor)을 이해해야 합니다
무작정 튜닝하기보다, 현재 장비의 'Identity Limit(순수 I/O 속도)'을 먼저 측정하는 것이 목표 설정에 큰 도움이 되었습니다. 8초라는 기준점이 있었기에 24초가 얼마나 최적화된 수치인지 확신할 수 있었습니다.

## 4. 마치며

이번 프로젝트는 단순히 "빨리 처리하는 것" 이상의 의미가 있었습니다. 데이터의 크기(100M)와 도구(Spark/DuckDB), 그리고 하드웨어의 임계점 사이의 균형을 맞추는 데이터 엔지니어링의 정수를 경험할 수 있었습니다.

11분의 대장정을 24초의 찰나로 바꾼 이 기술적 근육들은, 앞으로 더 복잡한 금융 데이터를 처리하는 든든한 기반이 될 것입니다.

---
**최종 결과 요약**
- **처리량**: 100,000,000건
- **최종 시간**: 24.65초 (Spark 대비 27.8배 개선)
- **핵심 기술**: Medallion Architecture, Native C++ Vectorization, Hash Optimization, I/O Alignment
